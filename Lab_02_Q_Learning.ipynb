{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZHAWSleeping/MachineLearning_FS_23/blob/main/Lab_02_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cbhF6-H5x_b7"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDr7ZGY-x_b9"
      },
      "source": [
        "# Q-Learning on the CartPole Environment\n",
        "\n",
        "This is an altered version of Jose Nieves Flores Maynez' notebook.\n",
        "\n",
        "This tutorial shows how to use Q-Learning to train an RL agent on the CartPole-v0 task from the [OpenAI Gym](https://gym.openai.com/).\n",
        "\n",
        "![cartpole](https://github.com/pytorch/tutorials/blob/main/_static/img/cartpole.gif?raw=true)\n",
        "\n",
        "The Cartpole environment is a common simple example that is used often for simple RL examples.\n",
        "\n",
        "In this environment, the task is to balance the pole that is attached to the cart, by moving the cart to either side.\n",
        "The reward gets incremented for each step (for up to 200 steps) where the pole is not exceeding a set angle and the cart is not touching the sides of the line.\n",
        "The environment provides four parameters that represent the state of the environment:\n",
        "Position and velocity of the cart and angle and angular velocity of the pole (see [the documentation](https://gymnasium.farama.org/environments/classic_control/cart_pole/#observation-space)).\n",
        "We will solve this by applying Q-Learning to our RL agent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Packages\n",
        "\n",
        "\n",
        "First, let's import needed packages."
      ],
      "metadata": {
        "id": "QCWbuk66234H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9ccdmXWUx_b_"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CdY_ONkqSFL6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psxbGvWJx_cB"
      },
      "source": [
        "## Implementation\n",
        "Since this algorithm relies on updating a function for each existing pair of state and action, environments that have a high state-space become problematic. This is because we can approximate better the actual value of a state-action pair as we visit it more often. However, if we have many states or many actions to take, we distribute our visits among more pairs and it takes much longer to converge to the actual true values. The CartPole environment gives us the position of the cart, its velocity, the angle of the pole and the velocity at the tip of the pole as descriptors of the state. However, all of these are continuous variables. To be able to solve this problem, we need to discretize these states since otherwise, it would take forever to get values for each of the possible combinations of each state, despite them being bounded. The solution is to group several values of each of the variables into the same “bucket” and treat them as similar states. The agent implemented for this problem uses 3, 3, 6, and 6 buckets respectively."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Base code taken from:\n",
        "https://github.com/IsaacPatole/CartPole-v0-using-Q-learning-SARSA-and-DNN/blob/master/Qlearning_for_cartpole.py\n",
        "\"\"\"\n",
        "\n",
        "class CartPoleQAgent():\n",
        "    def __init__(self, buckets=(3, 3, 6, 6),\n",
        "                 num_episodes=500, min_lr=0.1,\n",
        "                 min_epsilon=0.1, discount=1.0, decay=25):\n",
        "        self.buckets = buckets\n",
        "        self.num_episodes = num_episodes\n",
        "        self.min_lr = min_lr\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.discount = discount\n",
        "        self.decay = decay\n",
        "\n",
        "\n",
        "        self.env = gym.make('CartPole-v0')\n",
        "        self.theta = np.random.rand((self.env.action_space.n), 1)\n",
        "\n",
        "        # This is the action-value function being initialized to 0's\n",
        "        self.Q_table = np.zeros(self.buckets + (self.env.action_space.n,))\n",
        "\n",
        "        # [position, velocity, angle, angular velocity]\n",
        "        self.upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50) / 1.]\n",
        "        self.lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50) / 1.]\n",
        "\n",
        "        #\n",
        "        self.steps = np.zeros(self.num_episodes)\n",
        "\n",
        "    # Pi function (Policy)\n",
        "    def logistic_policy(self, state):\n",
        "        array = []\n",
        "        for a in range(self.env.action_space.n):\n",
        "            array.append(1/(1 + np.exp(-self.theta[a].T) * state))\n",
        "        return array\n",
        "\n",
        "    # Logistic policy gradient\n",
        "    def gradient(self, state):\n",
        "        array = []\n",
        "        for a in range(self.env.action_space.n):\n",
        "            array.append(state - state * self.logistic_policy(self, state)[a])\n",
        "        return array\n",
        "    # G function to estimate return\n",
        "    def G_function(self, step, maxSteps, reward):\n",
        "        result = 0\n",
        "        for k in range(step, maxSteps):\n",
        "            result += self.decay**(k - step - 1) * reward[k]\n",
        "        return result\n",
        "\n",
        "    # updates parameters\n",
        "    def update_Theta(self, step, state, G):\n",
        "        return self.theta.T + self.get_learning_rate(step) * self.decay**step * G * self.gradient(state)\n",
        "\n",
        "\n",
        "\n",
        "    def discretize_state(self, obs):\n",
        "        \"\"\"\n",
        "        Takes an observation of the environment and aliases it.\n",
        "        By doing this, very similar observations can be treated\n",
        "        as the same and it reduces the state space so that the\n",
        "        Q-table can be smaller and more easily filled.\n",
        "\n",
        "        Input:\n",
        "        obs (tuple): Tuple containing 4 floats describing the current\n",
        "                     state of the environment.\n",
        "\n",
        "        Output:\n",
        "        discretized (tuple): Tuple containing 4 non-negative integers smaller\n",
        "                             than n where n is the number in the same position\n",
        "                             in the buckets list.\n",
        "        \"\"\"\n",
        "        discretized = list()\n",
        "        for i in range(len(obs)):\n",
        "            scaling = ((obs[i] + abs(self.lower_bounds[i]))\n",
        "                       / (self.upper_bounds[i] - self.lower_bounds[i]))\n",
        "            new_obs = int(round((self.buckets[i] - 1) * scaling))\n",
        "            new_obs = min(self.buckets[i] - 1, max(0, new_obs))\n",
        "            discretized.append(new_obs)\n",
        "        return tuple(discretized)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"\n",
        "        Implementation of e-greedy algorithm. Returns an action (0 or 1).\n",
        "\n",
        "        Input:\n",
        "        state (tuple): Tuple containing 4 non-negative integers within\n",
        "                       the range of the buckets.\n",
        "\n",
        "        Output:\n",
        "        (int) Returns either 0 or 1\n",
        "        \"\"\"\n",
        "        #if (np.random.random() < self.epsilon):\n",
        "         #   return self.env.action_space.sample()\n",
        "        #else:\n",
        "            #return np.argmax(self.Q_table[state])\n",
        "        return np.argmax(self.logistic_policy(state))\n",
        "\n",
        "    def get_action(self, state, e):\n",
        "        \"\"\"\n",
        "        Another policy based on the Q-table. Slight variation from\n",
        "        e-greedy. It assumes the state fed hasn't been discretized and\n",
        "        returns a vector with probabilities for each action.\n",
        "\n",
        "        Input:\n",
        "        state (tuple): Contains the 4 floats used to describe\n",
        "                       the current state of the environment.\n",
        "        e (int): Denotes the episode at which the agent is supposed\n",
        "                 to be, helping balance exploration and exploitation.\n",
        "\n",
        "        Output:\n",
        "        action_vector (numpy array): Vector containing the probability\n",
        "                                     of each action being chosen at the\n",
        "                                     current state.\n",
        "        \"\"\"\n",
        "        obs = self.discretize_state(state)\n",
        "        action_vector = self.Q_table[obs]\n",
        "        epsilon = self.get_epsilon(e)\n",
        "        action_vector = self.normalize(action_vector, epsilon)\n",
        "        return action_vector\n",
        "\n",
        "    def normalize(self, action_vector, epsilon):\n",
        "        \"\"\"\n",
        "        Returns a vector with components adding to 1. Ensures\n",
        "\n",
        "        Input:\n",
        "        action_vector (numpy array): Contains expected values for each\n",
        "                                     action at current state from Q-table.\n",
        "        epsilon (float): Chances that the e-greedy algorithm would\n",
        "                         choose an action at random. With this pol\n",
        "\n",
        "        Output:\n",
        "        new_vector (numpy array): Vector containing the probability\n",
        "                                  of each action being chosen at the\n",
        "                                  current state.\n",
        "        \"\"\"\n",
        "\n",
        "        total = sum(action_vector)\n",
        "        new_vector = (1-epsilon)*action_vector/(total)\n",
        "        new_vector += epsilon/2.0\n",
        "        return new_vector\n",
        "\n",
        "    def update_q(self, state, action, reward, new_state):\n",
        "        \"\"\"\n",
        "        Updates Q-table using the rule as described by Sutton and Barto in\n",
        "        Reinforcement Learning.\n",
        "        \"\"\"\n",
        "        self.Q_table[state][action] += (self.learning_rate *\n",
        "                                        (reward\n",
        "                                         + self.discount * np.max(self.Q_table[new_state])\n",
        "                                         - self.Q_table[state][action]))\n",
        "\n",
        "    def get_epsilon(self, t):\n",
        "        \"\"\"Gets value for epsilon. It declines as we advance in episodes.\"\"\"\n",
        "        # Ensures that there's almost at least a min_epsilon chance of randomly exploring\n",
        "        return max(self.min_epsilon, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
        "\n",
        "    def get_learning_rate(self, t):\n",
        "        \"\"\"Gets value for learning rate. It declines as we advance in episodes.\"\"\"\n",
        "        # Learning rate also declines as we add more episodes\n",
        "        return max(self.min_lr, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Trains agent making it go through the environment and choose actions\n",
        "        through an e-greedy policy and updating values for its Q-table. The\n",
        "        agent is trained by default for 500 episodes with a declining\n",
        "        learning rate and epsilon values that with the default values,\n",
        "        reach the minimum after 198 episodes.\n",
        "        \"\"\"\n",
        "        # Looping for each episode\n",
        "        for e in range(self.num_episodes):\n",
        "            # Initializes the state\n",
        "            current_state = self.discretize_state(self.env.reset())\n",
        "\n",
        "            state_array = []\n",
        "            state_array.append(current_state)\n",
        "\n",
        "            self.learning_rate = self.get_learning_rate(e)\n",
        "            self.epsilon = self.get_epsilon(e)\n",
        "\n",
        "            rewards_array = []\n",
        "            done = False\n",
        "\n",
        "            # generate trajectory\n",
        "            while not done:\n",
        "                \"\"\"\n",
        "                self.steps[e] += 1\n",
        "                # Choose A from S\n",
        "                action = self.choose_action(current_state)\n",
        "                # Take action\n",
        "                obs, reward, done, _ = self.env.step(action)\n",
        "                new_state = self.discretize_state(obs)\n",
        "                # Update Q(S,A)\n",
        "                self.update_q(current_state, action, reward, new_state)\n",
        "                current_state = new_state\n",
        "\n",
        "                # We break out of the loop when done is False which is\n",
        "                # a terminal state.\n",
        "                \"\"\"\n",
        "                self.steps[e] += 1\n",
        "                action = self.choose_action(current_state)\n",
        "                obs, reward, done, _ = self.env.step(action)\n",
        "                new_state = self.discretize_state(obs)\n",
        "                current_state = new_state\n",
        "                state_array.append(current_state)\n",
        "                rewards_array.append(reward)\n",
        "\n",
        "                print(self.steps[e])\n",
        "            for step in range(self.steps[e]):\n",
        "                G = self.G_function(step, len(self.steps[e]), rewards_array[step])\n",
        "                self.update_Theta(step, state_array[step], G)\n",
        "\n",
        "        print('Finished training!')\n",
        "\n",
        "    def plot_learning(self):\n",
        "        \"\"\"\n",
        "        Plots the number of steps at each episode and prints the\n",
        "        amount of times that an episode was successfully completed.\n",
        "        \"\"\"\n",
        "        sns.lineplot(data=self.steps)\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Steps\")\n",
        "        plt.show()\n",
        "        t = 0\n",
        "        for i in range(self.num_episodes):\n",
        "            if self.steps[i] == 200:\n",
        "                t+=1\n",
        "        print(t, \"episodes were successfully completed.\")\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Runs an episode while displaying the cartpole environment.\"\"\"\n",
        "        self.env = gym.wrappers.Monitor(self.env,'cartpole')\n",
        "        t = 0\n",
        "        done = False\n",
        "        current_state = self.discretize_state(self.env.reset())\n",
        "        while not done:\n",
        "                self.env.render()\n",
        "                t = t+1\n",
        "                action = self.choose_action(current_state)\n",
        "                obs, reward, done, _ = self.env.step(action)\n",
        "                new_state = self.discretize_state(obs)\n",
        "                current_state = new_state\n",
        "\n",
        "        return t\n",
        "\n",
        "\n",
        "\n",
        "def load_q_learning():\n",
        "    agent = CartPoleQAgent()\n",
        "    agent.train()\n",
        "    agent.plot_learning()\n",
        "\n",
        "    return agent\n",
        "\n",
        "agent = load_q_learning()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "tqqVqJYZwjQr",
        "outputId": "716bc4bf-c6b3-4146-8c30-b77e830dee3f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d4f10d374ded>\u001b[0m in \u001b[0;36m<cell line: 257>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_q_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-d4f10d374ded>\u001b[0m in \u001b[0;36mload_q_learning\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_q_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCartPoleQAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-d4f10d374ded>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m                 \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscretize_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_step_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep_to_new_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py\u001b[0m in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;34m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;31m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     assert isinstance(\n\u001b[1;32m    224\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{action!r} ({type(action)}) invalid\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Call reset before using step method.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: 4 (<class 'numpy.int64'>) invalid"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hA9M6GBuoM08"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}